{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'functions' from 'c:\\\\Applied AI\\\\Applied_AI\\\\Applied AI project\\\\applied_ai_project\\\\functions.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import pandas as pd \n",
    "import functions as func\n",
    "import importlib\n",
    "importlib.reload(func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self,num_features,num_targets):\n",
    "        super(ClassificationNN, self).__init__()\n",
    "\n",
    "        self.num_featurs = num_features\n",
    "        self.num_targets = num_targets\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_features,128)\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "\n",
    "        if num_targets == 2:\n",
    "            self.output = nn.Linear(128,1)\n",
    "        else:\n",
    "            self.output = nn.Linear(128,num_targets)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "            \n",
    "        if self.num_targets == 2:\n",
    "            x = torch.sigmoid(x)\n",
    "        #No need if num_targets is not binary (cross entropy loss applied from the outside)\n",
    "        \n",
    "        return x \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_sv_classification_model(dataset,epochs,n_splits):\n",
    "\n",
    "    data,target,num_targets,num_features = func.read_data(dataset)\n",
    "\n",
    "    model = ClassificationNN(num_features, num_targets=num_targets)\n",
    "    learning_rate=0.001\n",
    "\n",
    "    model.train()\n",
    "    running_loss=0\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(data.drop(target,axis=1), data[target], test_size=0.33, random_state=42)\n",
    "\n",
    "    skf = KFold(n_splits=n_splits)\n",
    "\n",
    "    fold = 0\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for train_index, val_index in skf.split(X_train_val,y_train_val):\n",
    "        X_train,X_val = func.convert_df_to_tensor([X_train_val.iloc[train_index],X_train_val.iloc[val_index]])\n",
    "        y_train,y_val = func.convert_df_to_tensor([y_train_val.iloc[train_index],y_train_val.iloc[val_index]])\n",
    "\n",
    "        train_loader = func.convert_to_dataloader(X_train,y_train)\n",
    "        val_loader = func.convert_to_dataloader(X_val,y_val)\n",
    "\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        \n",
    "        if num_targets == 2:\n",
    "            criterion = nn.BCELoss() \n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(1,epochs+1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                if num_targets == 2:\n",
    "                    loss = criterion(outputs, batch_y.unsqueeze(1).float())\n",
    "                else:\n",
    "                    loss = criterion(outputs, batch_y.long())\n",
    "\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            \n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(f'Epoch [{epoch}/{epochs}], Loss: {avg_loss:.4f}')            \n",
    "        \n",
    "        #Test model\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total =0 \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x,batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "\n",
    "                if num_targets == 2:\n",
    "                    \n",
    "                    predicted = (outputs > 0.5).float()\n",
    "                    correct += (predicted.squeeze(1) == batch_y.float()).sum().item()\n",
    "                else:\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs, dim=1)\n",
    "                    correct += (predicted == batch_y.long()).sum().item()\n",
    "                total += batch_y.size(0)\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Fold {fold} Accuracy: {accuracy:.2f}% for dataset {dataset}')\n",
    "        fold_accuracies.append(accuracy)\n",
    "        fold += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_sv_classification_model(dataset,model):\n",
    "\n",
    "    data,target,num_targets,num_features = func.read_data(dataset)\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(data.drop(target,axis=1), data[target], test_size=0.33, random_state=42)\n",
    "\n",
    "    X_test = func.convert_df_to_tensor(X_test)\n",
    "    y_test = func.convert_df_to_tensor(y_test)\n",
    "\n",
    "    eval_loader = func.convert_to_dataloader(X_test,y_test)\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "                   \n",
    "    #Test model\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total =0 \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x,batch_y in eval_loader:\n",
    "            batch_X, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "\n",
    "            if num_targets == 2:\n",
    "                \n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct += (predicted.squeeze(1) == batch_y.float()).sum().item()\n",
    "            else:\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                correct += (predicted == batch_y.long()).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}% for dataset {dataset}')\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 1.7377\n",
      "Epoch [10/30], Loss: 0.2651\n",
      "Epoch [20/30], Loss: 0.3144\n",
      "Epoch [30/30], Loss: 0.1966\n",
      "Fold 0 Accuracy: 35.33% for dataset ecoli\n",
      "Epoch [1/30], Loss: 0.3445\n",
      "Epoch [10/30], Loss: 0.2783\n",
      "Epoch [20/30], Loss: 0.2186\n",
      "Epoch [30/30], Loss: 0.2561\n",
      "Fold 1 Accuracy: 59.00% for dataset ecoli\n",
      "Epoch [1/30], Loss: 0.2791\n",
      "Epoch [10/30], Loss: 0.2640\n",
      "Epoch [20/30], Loss: 0.2934\n",
      "Epoch [30/30], Loss: 0.3038\n",
      "Fold 2 Accuracy: 40.42% for dataset ecoli\n",
      "Epoch [1/30], Loss: 0.3806\n",
      "Epoch [10/30], Loss: 0.3383\n",
      "Epoch [20/30], Loss: 0.2884\n",
      "Epoch [30/30], Loss: 0.2277\n",
      "Fold 3 Accuracy: 36.85% for dataset ecoli\n",
      "Epoch [1/30], Loss: 0.4639\n",
      "Epoch [10/30], Loss: 0.2584\n",
      "Epoch [20/30], Loss: 0.3508\n",
      "Epoch [30/30], Loss: 0.3314\n",
      "Fold 4 Accuracy: 62.34% for dataset ecoli\n",
      "Epoch [1/30], Loss: 0.7042\n",
      "Epoch [10/30], Loss: 0.4320\n",
      "Epoch [20/30], Loss: 0.4192\n",
      "Epoch [30/30], Loss: 0.4080\n",
      "Fold 0 Accuracy: 48.65% for dataset mammographic\n",
      "Epoch [1/30], Loss: 0.4359\n",
      "Epoch [10/30], Loss: 0.4022\n",
      "Epoch [20/30], Loss: 0.4070\n",
      "Epoch [30/30], Loss: 0.4032\n",
      "Fold 1 Accuracy: 45.05% for dataset mammographic\n",
      "Epoch [1/30], Loss: 0.4090\n",
      "Epoch [10/30], Loss: 0.4032\n",
      "Epoch [20/30], Loss: 0.3855\n",
      "Epoch [30/30], Loss: 0.3885\n",
      "Fold 2 Accuracy: 57.66% for dataset mammographic\n",
      "Epoch [1/30], Loss: 0.4183\n",
      "Epoch [10/30], Loss: 0.3947\n",
      "Epoch [20/30], Loss: 0.3700\n",
      "Epoch [30/30], Loss: 0.3661\n",
      "Fold 3 Accuracy: 54.05% for dataset mammographic\n",
      "Epoch [1/30], Loss: 0.4153\n",
      "Epoch [10/30], Loss: 0.3689\n",
      "Epoch [20/30], Loss: 0.3721\n",
      "Epoch [30/30], Loss: 0.3701\n",
      "Fold 4 Accuracy: 49.55% for dataset mammographic\n",
      "Epoch [1/30], Loss: 1.2414\n",
      "Epoch [10/30], Loss: 0.5073\n",
      "Epoch [20/30], Loss: 0.3386\n",
      "Epoch [30/30], Loss: 0.2406\n",
      "Fold 0 Accuracy: 46.43% for dataset seeds\n",
      "Epoch [1/30], Loss: 0.2128\n",
      "Epoch [10/30], Loss: 0.2573\n",
      "Epoch [20/30], Loss: 0.2011\n",
      "Epoch [30/30], Loss: 0.2244\n",
      "Fold 1 Accuracy: 21.43% for dataset seeds\n",
      "Epoch [1/30], Loss: 0.2755\n",
      "Epoch [10/30], Loss: 0.2196\n",
      "Epoch [20/30], Loss: 0.2441\n",
      "Epoch [30/30], Loss: 0.2284\n",
      "Fold 2 Accuracy: 25.00% for dataset seeds\n",
      "Epoch [1/30], Loss: 0.3118\n",
      "Epoch [10/30], Loss: 0.2563\n",
      "Epoch [20/30], Loss: 0.2199\n",
      "Epoch [30/30], Loss: 0.2752\n",
      "Fold 3 Accuracy: 32.14% for dataset seeds\n",
      "Epoch [1/30], Loss: 0.2305\n",
      "Epoch [10/30], Loss: 0.1849\n",
      "Epoch [20/30], Loss: 0.1620\n",
      "Epoch [30/30], Loss: 0.1916\n",
      "Fold 4 Accuracy: 42.86% for dataset seeds\n",
      "Epoch [1/10], Loss: 1.7367\n",
      "Epoch [10/10], Loss: 1.1501\n",
      "Fold 0 Accuracy: 31.11% for dataset yeast\n",
      "Epoch [1/10], Loss: 1.1333\n",
      "Epoch [10/10], Loss: 1.0512\n",
      "Fold 1 Accuracy: 31.92% for dataset yeast\n",
      "Epoch [1/30], Loss: 12.5591\n",
      "Epoch [10/30], Loss: 6.1741\n",
      "Epoch [20/30], Loss: 6.1741\n",
      "Epoch [30/30], Loss: 6.1741\n",
      "Fold 0 Accuracy: 94.35% for dataset ozone\n",
      "Epoch [1/30], Loss: 6.4712\n",
      "Epoch [10/30], Loss: 6.4712\n",
      "Epoch [20/30], Loss: 6.4712\n",
      "Epoch [30/30], Loss: 6.4712\n",
      "Fold 1 Accuracy: 95.55% for dataset ozone\n",
      "Epoch [1/30], Loss: 5.2578\n",
      "Epoch [10/30], Loss: 5.2578\n",
      "Epoch [20/30], Loss: 5.2578\n",
      "Epoch [30/30], Loss: 5.2578\n",
      "Fold 2 Accuracy: 90.69% for dataset ozone\n",
      "Epoch [1/30], Loss: 6.2690\n",
      "Epoch [10/30], Loss: 6.2690\n",
      "Epoch [20/30], Loss: 6.2690\n",
      "Epoch [30/30], Loss: 6.2690\n",
      "Fold 3 Accuracy: 94.74% for dataset ozone\n",
      "Epoch [1/30], Loss: 6.1678\n",
      "Epoch [10/30], Loss: 6.1678\n",
      "Epoch [20/30], Loss: 6.1678\n",
      "Epoch [30/30], Loss: 6.1678\n",
      "Fold 4 Accuracy: 94.33% for dataset ozone\n"
     ]
    }
   ],
   "source": [
    "datasets = {'ecoli':[30,5],'mammographic':[30,5],'seeds':[30,5],'yeast':[10,2],'ozone':[30,5]}\n",
    "model_dict = {}\n",
    "for dataset_name,param_list in datasets.items():\n",
    "    model_dict[dataset_name] = train_sv_classification_model(dataset_name,*param_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.56% for dataset ecoli\n",
      "Accuracy: 85.40% for dataset mammographic\n",
      "Accuracy: 85.71% for dataset seeds\n",
      "Accuracy: 55.33% for dataset yeast\n",
      "Accuracy: 91.31% for dataset ozone\n"
     ]
    }
   ],
   "source": [
    "accuracy_dict = {}\n",
    "for dataset,model in model_dict.items():\n",
    "    accuracy_dict[dataset] = evaluate_sv_classification_model(dataset,model)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
